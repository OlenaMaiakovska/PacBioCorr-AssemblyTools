{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Read Correction: Hybrid correctors (short and long reads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster node configuration:\n",
    "• Number of CPU: 40\n",
    "• CPU frequency: 2.6 GHz\n",
    "• Available RAM: 256 GBytes\n",
    "\n",
    "| Correctors | Number of threads |\n",
    "|------------|-------------------|\n",
    "| LSC 2      | 8                 |\n",
    "|  Proovread | 8                 |\n",
    "| PacbiotoCa  | 8                 |\n",
    "| Lordec     | 8                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LSC-2\n",
    "#### Introduction\n",
    "LSC-2 is an hybrid corrector of long reads. Long reads and short reads are first compressed into\n",
    "homopolymers, then short reads are mapped to long reads with Bowtie2. Finally, the short read\n",
    "consensus replaces the long read sequences.\n",
    "Website: http://www.healthcare.uiowa.edu/labs/au/LSC/default.asp\n",
    "\n",
    "#### Installation\n",
    "LSC-2 can be downloaded as pre-compiled binaries. Bowtie2 is required and must be installed.\n",
    "Extraction of pre-compiled binaries files:\n",
    "$ tar zxvf LSC-2.0.tar.gz\n",
    "\n",
    "#### Input data\n",
    "LSC-2 takes FASTA or FASTQ files as input.\n",
    "#### Pipeline\n",
    "+ runLSC script divides long read error correction into 5 steps:\n",
    "    - The sequences of long and the short reads are transformed by homopolymer compression\n",
    "so that each sequence of the same nucleotide is replaced by a single nucleotide of the same\n",
    "type.\n",
    "    - The short reads quality is checked. Indeed, some of these reads contain too much N letters\n",
    "or are too short.\n",
    "    - The short reads are aligned against the long ones with Bowtie2.\n",
    "    - The long reads are then modified according to the information provided by the short read\n",
    "consensus obtained with previous alignment.\n",
    "    - Once the correction points have been replaced by the corresponding short reads consensus,\n",
    "the rest of the compressed points are decompressed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runLSC.py: command not found\n"
     ]
    },
    {
     "ename": "",
     "evalue": "127",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "runLSC.py --long_reads LR.fa --short_reads SR.fa --specific_tempdir temp --output output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• longs reads : long read file.\n",
    "• short reads : short read file.\n",
    "• specific tempdir : folder containing temporary files (optional).\n",
    "• output : final assembly folder.\n",
    "\n",
    "#### Encountered errors\n",
    "Died at /LSC-2.0/bin/../utilities/explode fasta.pl\n",
    "ValueError: invalid literal for int() with base 10: ”\n",
    "    - solution : Convert short reads from FASTQ format to FASTA format (AWK command line or Biopython Seq.IO module)\n",
    "</p> [bam header read] EOF marker is absent. The input is probably truncated.\n",
    "    - solution : Install Bowtie2.\n",
    "#### Output data\n",
    "The corrected sequences are written into the ”corrected read.fasta” file, while full LR.fasta file\n",
    "contains concatenate uncorrected terminus sequences and corrected sequences. Both files are\n",
    "located in the final assembly folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Proovread\n",
    "#### Introduction\n",
    "Proovread is a de novo corrector, using a de-Bruijn graph constructed from long reads.\n",
    "Website : https://github.com/BioInf-Wuerzburg/proovread\n",
    "#### Installation\n",
    "Proovread is available on Linux and requires NCBI Blast-2.2.24+, samtools-1.1+, Perl 5.10.1+\n",
    "and the perl moduls Log::Log4perl and File::Which.\n",
    "#### Code source compilation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ git clone --recursive https://github.com/BioInf-Wuerzburg/proovread\n",
    "$ cd proovread/util/bwa\n",
    "$ make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input data\n",
    "In order to not overload the processor and the memory, it is wise to divide the long reads data\n",
    "(FASTA, FASTQ) into several files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ SeqChunker -s 20M -o 0%03d pacbio_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - s : file length\n",
    "    - o : output file name\n",
    "#### Pipeline\n",
    "Run the long read error correction process with the binary ”proovread”, located in the folder\n",
    "”bin”, for every folders created in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ for i in {0001..000n}; do proovread -l $i -s /Path/to/short_reads {pre pb_$i; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - n : Number of files generated by SeqChunker\n",
    "    - l : raw noisy long reads\n",
    "    - s : file containing accurate short reads\n",
    "    - pre : prefix used to name output file.\n",
    "It is also possible to add unitigs with the argument ”-unitigs”.\n",
    "+ Proovread corrects long reads in 2 steps:\n",
    "    - The mapping of short reads on long reads is done by SHRIMP2, by adapting the score\n",
    "mode to consider that insertions are more frequent than deletions and that substitutions\n",
    "are rare events. Bowtie2 and bwa mem are also supported.\n",
    "    - A consensus sequence is computed from these alignments.\n",
    "    \n",
    "#### Output data\n",
    "The corrected sequences are written in the specified output folder (trimmed and untrimmed\n",
    "reads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LoRDEC\n",
    "#### Introduction\n",
    "LoRDEC is an hybrid corrector, using a de-Bruijn graph constructed from short reads to correct\n",
    "long reads. Website : http://www.atgc-montpellier.fr/lordec/\n",
    "#### Installation\n",
    "LoRDEC is available on linux and requires Cmake 2.6+ and GCC 4.7+.\n",
    "+ Import LoRDEC and the GATB library (http://gatb-core.gforge.inria.fr/) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-01-08 11:02:25--  http://www.atgc-montpellier.fr/download/sources/lordec/LoRDEC-0.6.tar.gz\n",
      "Resolving www-int2.inet.dkfz-heidelberg.de (www-int2.inet.dkfz-heidelberg.de)... 193.174.53.221, 193.174.53.86\n",
      "Connecting to www-int2.inet.dkfz-heidelberg.de (www-int2.inet.dkfz-heidelberg.de)|193.174.53.221|:80... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 1224978 (1,2M) [application/x-gzip]\n",
      "Saving to: ‘LoRDEC-0.6.tar.gz’\n",
      "\n",
      "LoRDEC-0.6.tar.gz   100%[===================>]   1,17M  1,05MB/s    in 1,1s    \n",
      "\n",
      "2018-01-08 11:02:26 (1,05 MB/s) - ‘LoRDEC-0.6.tar.gz’ saved [1224978/1224978]\n",
      "\n",
      "LoRDEC-0.6/\n",
      "LoRDEC-0.6/DATA/\n",
      "LoRDEC-0.6/DATA/tau-pacbio-test.fa.gz\n",
      "LoRDEC-0.6/DATA/coli-pacbio-test-mini.fa.gz\n",
      "LoRDEC-0.6/DATA/ill-test-5K-1.h5\n",
      "LoRDEC-0.6/DATA/ill-test-5K-2.h5\n",
      "LoRDEC-0.6/DATA/my2-ill-test-files.h5\n",
      "LoRDEC-0.6/DATA/ill-test-5K-1.fa\n",
      "LoRDEC-0.6/DATA/ill-test-5K-2.fa\n",
      "LoRDEC-0.6/DATA/pacbio-test.fa.gz\n",
      "LoRDEC-0.6/DATA/ill-test-5K-2.fa,ill-test-5K-1.h5\n",
      "LoRDEC-0.6/DATA/my2-ill-test-files.txt\n",
      "LoRDEC-0.6/Makefile\n",
      "LoRDEC-0.6/LICENSE\n",
      "LoRDEC-0.6/lordec-trim-split.cpp\n",
      "LoRDEC-0.6/lordec-gen.hpp\n",
      "LoRDEC-0.6/lordec-build-SR-graph.cpp\n",
      "LoRDEC-0.6/README.html\n",
      "LoRDEC-0.6/lordec-stat.cpp\n",
      "LoRDEC-0.6/README.org\n",
      "LoRDEC-0.6/test-lordec.sh\n",
      "LoRDEC-0.6/lordec-correct.cpp\n",
      "LoRDEC-0.6/lordec-trim.cpp\n",
      "--2018-01-08 11:02:27--  https://github.com/GATB/gatb-core/releases/download/v1.1.0/\n",
      "Resolving www-int2.inet.dkfz-heidelberg.de (www-int2.inet.dkfz-heidelberg.de)... 193.174.53.86, 193.174.53.221\n",
      "Connecting to www-int2.inet.dkfz-heidelberg.de (www-int2.inet.dkfz-heidelberg.de)|193.174.53.86|:80... connected.\n",
      "Proxy request sent, awaiting response... 404 Not Found\n",
      "2018-01-08 11:02:28 ERROR 404: Not Found.\n",
      "\n",
      "--2018-01-08 11:02:28--  http://gatb-core-1.1.0-bin-linux.tar.gz/\n",
      "Connecting to www-int2.inet.dkfz-heidelberg.de (www-int2.inet.dkfz-heidelberg.de)|193.174.53.86|:80... connected.\n",
      "Proxy request sent, awaiting response... 503 Service Unavailable\n",
      "2018-01-08 11:02:28 ERROR 503: Service Unavailable.\n",
      "\n",
      "tar (child): gatb-core-1.1.0-bin-Linux.tar.gz: Cannot open: No such file or directory\n",
      "tar (child): Error is not recoverable: exiting now\n",
      "tar: Child returned status 2\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    },
    {
     "ename": "",
     "evalue": "2",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "$ wget http://www.atgc-montpellier.fr/download/sources/lordec/LoRDEC-0.6.tar.gz\n",
    "$ tar zxvf LoRDEC-0.6.tar.gz\n",
    "$ cd LoRDEC-0.6\n",
    "$ wget https://github.com/GATB/gatb-core/releases/download/v1.1.0/ \\gatb-core-1.1.0-bin-Linux.tar.gz\n",
    "$ tar zxvf gatb-core-1.1.0-bin-Linux.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the variable GATB VER from the Makefile (1.1.0) Install LoRDEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ make\n",
    "$ cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input data\n",
    "LoRDEC requires short reads in FASTA or FASTQ file format and long reads in FASTA or\n",
    "FASTQ file format.\n",
    "#### Pipeline\n",
    "Run the long read error correction with the binary ”lordec-correct”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ lordec-correct -2 illumina.fasta -k 19 -s 3 -i pacbio.fasta \\\n",
    "-o pacbio-corrected.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - 2 : File of short reads.\n",
    "    - k : Size of the kmer used in the de-Bruijn graph\n",
    "    - s : Abundance threshold of a kmer to be considered correct\n",
    "    - i : Input file\n",
    "    - o : Output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series of steps is then performed in order to correct the long reads:\n",
    "     1. Construction of a de-Bruijn graph from the short reads\n",
    "     2. Suppression of k-mer with occurrence less than the s value\n",
    "     3. Choose an optimal path of the graph by calculating the edit distance between the path and a region of long read.\n",
    "#### Output data\n",
    "The corrected sequences will be in the output file indicated after the ”-o” parameter. The output\n",
    "file in FASTA format contains long reads. Corrected sequences are defined by uppercase letters\n",
    "while uncorrected sequences appears as lowercase letters. Lordec offers the possibility to remove\n",
    "the uncorrected sequences at the beginning and at the end of the long reading or to keep only\n",
    "the corrected sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ lordec-trim -i fichier_reads.fasta -o fichier_trim.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    i : corrected reads file\n",
    "    o : output file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ loredec-trim-split -i fichier_reads.fasta -o fichier_trim_split.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HALC\n",
    "#### Introduction\n",
    "HALC is software that makes error correction for long reads with high throughput.\n",
    "#### Installation\n",
    "Aligner BLASR and error correction software LoRDEC (only for -ordinary mode) are required to run HALC.\n",
    "\n",
    "    The source files in 'src' and 'thirdparty' folders can be compiled to generate a 'bin' folder by running Makefile: make all.\n",
    "    Put BLASR, LoRDEC and the 'bin' folder to your $PATH: export PATH=PATH2BLASR:$PATH , export PATH=PATH2LoRDEC:$PATH and export PATH=PATH2bin:$PATH, respectively.\n",
    "#### Input data\n",
    "\n",
    "    - Long reads in FASTA format.\n",
    "    - Contigs assembled from the corresponding short reads in FASTA format.\n",
    "    - The initial short reads in FASTA format (only for -ordinary mode; obtained with  cat left_reads.fa >short_reads.fa and  then cat right_reads.fa >>short_reads.fa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline\n",
    "runHALC.py long_reads.fa contigs.fa [-options|-options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ runHALC.py long_reads.fa contigs.fa -b 4 -a -w 2 -k 25 -t 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ runHALC.py long_reads.fa contigs.fa -o short_reads.fa -b 4 -a -w 2 -k 25 -t 8\n",
    "# or scaffolds instead of contigs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Error corrected full long reads.\n",
    "    Error corrected trimmed long reads.\n",
    "    Error corrected split long reads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PBJelly gapclosing\n",
    "PBJelly can be used to try to close or shrink gaps that may be present between contigs after scaffolding.\n",
    "### Requirements:\n",
    "\n",
    "\n",
    "    Blasr (https://github.com/PacificBiosciences/blasr)\n",
    "    Version 1.3.1.127046 is fully vetted as compatible with\n",
    "    Jelly. Other versions may run into problems. Use\n",
    "    > blasr -version\n",
    "    to figure out what you have. Blasr must be in your environment\n",
    "    path.\n",
    "\n",
    "    Python 2.7\n",
    "    Python must be in your environment path and executable with\n",
    "    the commands:\n",
    "    > python\n",
    "    > /usr/bin/env python\n",
    "\n",
    "    Networkx v1.1\n",
    "    Versions past v1.1 have been shown to have many issues. This will\n",
    "    be updated in the future. To check your version use, in a python\n",
    "    interactive terminal, type:\n",
    "    > import networkx\n",
    "    > networkx.version\n",
    "    If you get an error saying the attribute isn't found, you don't have\n",
    "    version 1.1 \n",
    "    \n",
    "    https://sourceforge.net/p/pb-jelly/wiki/Home/\n",
    "    https://github.com/alvaralmstedt/Tutorials/wiki/Gap-closing-with-PBJelly\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Assembly:\n",
    "This process is usually carried out by a pre-assembly of short fragments and subsequent joining of contigs using long reads to correctly order the sequences and fill gaps [5] ;  [6]. An opposite strategy is to employ long reads as a scaffold for assembly of short reads [7]; [8] ;  [9]. However, as concluded in one of the recent studies, the highest efficiency in genome assembly is obtained with approaches that are based on use of the high-accuracy Illumina sequencing data for correction of the PacBio SMRT or Nanopore sequencing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Canu\n",
    "#### Introduction\n",
    "is a fork of the Celera Assembler designed for high-noise single-molecule sequencing (such as the PacBio RSII or Oxford Nanopore MinION)\n",
    "\n",
    "#### Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "git clone https://github.com/marbl/canu.git\n",
    "cd canu/src\n",
    "make -j <number of threads>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canu [-correct | -trim | -assemble | -trim-assemble] \\\n",
    "  [-s <assembly-specifications-file>] \\\n",
    "   -p <assembly-prefix> \\\n",
    "   -d <assembly-directory> \\\n",
    "   genomeSize=<number>[g|m|k] \\\n",
    "   [other-options] \\\n",
    "   [-pacbio-raw | -pacbio-corrected | -nanopore-raw | -nanopore-corrected] *fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The -p option, to set the file name prefix of intermediate and output files, is mandatory. If -d is not supplied, canu will run in the current directory, otherwise, Canu will create the assembly-directory and run in that directory. It is _not_ possible to run two different assemblies in the same directory.\n",
    "\n",
    "The -s option will import a list of parameters from the supplied specification (‘spec’) file. These parameters will be applied before any from the command line are used, providing a method for setting commonly used parameters, but overriding them for specific assemblies.\n",
    "\n",
    "By default, all three top-level tasks are performed. It is possible to run exactly one task by using the -correct, -trim or -assemble options.  Additionally, suppling pre-corrected reads with -pacbio-corrected or -nanopore-corrected will run only the trimming (-trim) and assembling (-assemble) stages.\n",
    "\n",
    "One parameter is required: the genomeSize (in bases, with common SI prefixes allowed, for example, 4.7m or 2.8g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://canu.readthedocs.io/en/latest/_images/canu-pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canu -correct \\\n",
    "  -p ecoli -d ecoli \\\n",
    "  genomeSize=4.8m \\\n",
    "  -pacbio-raw  pacbio.fastq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, trim the output of the correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canu -trim \\\n",
    "  -p ecoli -d ecoli \\\n",
    "  genomeSize=4.8m \\\n",
    "  -pacbio-corrected ecoli/ecoli.correctedReads.fasta.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, assemble the output of trimming, twice, with different stringency on which overlaps to use (see correctedErrorRate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canu -assemble \\\n",
    "  -p ecoli -d ecoli-erate-0.039 \\\n",
    "  genomeSize=4.8m \\\n",
    "  correctedErrorRate=0.039 \\\n",
    "  -pacbio-corrected ecoli/ecoli.trimmedReads.fasta.gz\n",
    "\n",
    "canu -assemble \\\n",
    "  -p ecoli -d ecoli-erate-0.075 \\\n",
    "  genomeSize=4.8m \\\n",
    "  correctedErrorRate=0.075 \\\n",
    "  -pacbio-corrected ecoli/ecoli.trimmedReads.fasta.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the assembly stages use different ‘-d’ directories. It is not possible to run multiple copies of canu with the same work directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Consensus Accuracy\n",
    "\n",
    "Canu consensus sequences are typically well above 99% identity. Accuracy can be improved by polishing the contigs with tools developed specifically for that task. We recommend Quiver (https://github.com/PacificBiosciences/GenomicConsensus) for PacBio and Nanopolish for Oxford Nanpore data. When Illumina reads are available, Pilon (http://software.broadinstitute.org/software/pilon/) can be used to polish either PacBio or Oxford Nanopore assemblies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MaSuRCA\n",
    "#### Introduction\n",
    "assembler combines the benefits of deBruijn graph and Overlap Layout Consensus assembly approaches. \n",
    "Since version 3.2.1 itsupports hybrid assembly with short Illumina reads and long high error PacBio/MinION data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile/Install\n",
    "To compile the assembler we require gcc version 4.7 or newer to be installed on the system.\n",
    "The assembler has been tested on the following distributions:\n",
    "    - Fedora 12 and up\n",
    "    - RedHat 5 and 6 (requires installation of gcc 4.7)\n",
    "    - CentOS 5 and 6 (requires installation of gcc 4.7)\n",
    "    - Ubuntu 12 LTS and up\n",
    "    - SUSE Linux 16and up\n",
    "#### Hardware requirements\n",
    "The hardware requirements vary with the size of the genome project. Both Intel and AMD x64 architectures are\n",
    "supported. The general guidelines for hardware: Mammalian genomes (up \n",
    "to 3Gb): 512Gb RAM, 32+ cores, 5Tb disk space;\n",
    "Expected run times: Mammalian genomes (up to 3Gb): 15-20 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation\n",
    "To install, first download the latest distribution from \n",
    "ftp://ftp.genome.umd.edu/pub/MaSuRCA/\n",
    ". Then untar/unzip the package MaSuRCA\n",
    "-\n",
    "X.X.X.tgz, \n",
    "cd to the resulting folder and run './install.sh'.  The installation script will configure and make all \n",
    "necessary packages.\n",
    "In the rest of this document, '/install_path' refers to a path to the directory in which './install.sh' \n",
    "was run.\n",
    "\n",
    "IMPORTANT! Do not preprocess Illumina data before providing it to MaSuRCA.  Do not do any \n",
    "trimming, cleaning or error correction. This WILL deteriorate the assembly.\n",
    "\n",
    "First, \n",
    "create a configuration file which contains the location of the compiled assembler, the \n",
    "location of the data and\n",
    "some parameters. Copy in your assembly directory the template \n",
    "configuration file '/install_path/sr_config_example.txt' which was created by the \n",
    "installer with \n",
    "the correct paths to the freshly compiled software and with reasonable parameters. Many \n",
    "assembly projects should only need to set the path to the input data.\n",
    "Second, run the '\n",
    "masurca\n",
    "' script which will generate from the configuration file a\n",
    "shell script \n",
    "'assemble.sh'. This last script is the main driver of the assembly.\n",
    "Finally, run the script 'assemble.sh' to assemble the data.\n",
    "\n",
    "\n",
    "Configuration\n",
    ". To run the assembler, one must first create a configuration file that specifies the \n",
    "location of the executables, data and assembly parameters for the assembler. The installation \n",
    "script will create a sample config file 'sr_config_example.txt'. The sample configuration file looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ftp://ftp.genome.umd.edu/pub/MaSuRCA/MaSuRCA_QuickStartGuide.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: syntax error near unexpected token `('\n",
      "two_letter_prefix: command not found\n",
      "bash: syntax error near unexpected token `('\n",
      "---: command not found\n",
      "bash: syntax error near unexpected token `<'\n",
      "---: command not found\n",
      ",: command not found\n",
      "bash: syntax error near unexpected token `newline'\n",
      "---: command not found\n",
      "---: command not found\n",
      "bash: syntax error near unexpected token `;'\n",
      "libraries: command not found\n",
      "p,: command not found\n",
      "The program 'specify' is currently not installed. You can install it by typing:\n",
      "sudo apt install spectacle\n",
      "Illumina: command not found\n",
      "paired: command not found\n",
      "or: command not found\n",
      "reads: command not found\n",
      "JUMP: command not found\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: http://wgs: No such file or directory\n",
      "-: command not found\n",
      "assembler.sourceforge.com: command not found\n",
      "bash: syntax error near unexpected token `)'\n",
      "The program 'and' is currently not installed. You can install it by typing:\n",
      "sudo apt install and\n",
      "PAC: command not found\n",
      "BIO: command not found\n",
      "reads: command not found\n",
      "NANOPORE: command not found\n",
      "DATA: command not found\n",
      "pe: command not found\n",
      "sh: 0: Can't open 3600\n",
      "rt_1.fastq: command not found\n",
      "END: command not found\n",
      "PARAMETERS: command not found\n",
      "-: command not found\n",
      "No command 'mer' found, did you mean:\n",
      " Command 'mex' from package 'texlive-lang-polish' (universe)\n",
      " Command 'med' from package 'ncl-ncarg' (universe)\n",
      " Command 'mr' from package 'myrepos' (universe)\n",
      " Command 'mev' from package 'gpm' (universe)\n",
      " Command 'mlr' from package 'miller' (universe)\n",
      " Command 'mtr' from package 'mtr-tiny' (main)\n",
      " Command 'mtr' from package 'mtr' (universe)\n",
      "mer: command not found\n",
      "supported,: command not found\n",
      "The program 'and' is currently not installed. You can install it by typing:\n",
      "sudo apt install and\n",
      "set this longer than PE read lengthand GC content.  Do not!\n",
      "-: command not found\n",
      "only: command not found\n",
      "bash: syntax error near unexpected token `)'\n",
      "No command 'mate' found, did you mean:\n",
      " Command 'date' from package 'coreutils' (main)\n",
      " Command 'late' from package 'late' (universe)\n",
      " Command 'mute' from package 'aumix-common' (universe)\n",
      " Command 'mat' from package 'mat' (universe)\n",
      " Command 'mame' from package 'mame' (multiverse)\n",
      " Command 'matc' from package 'elmer' (universe)\n",
      " Command 'yate' from package 'yate' (universe)\n",
      " Command 'make' from package 'make' (main)\n",
      " Command 'make' from package 'make-guile' (universe)\n",
      " Command 'tmate' from package 'tmate' (universe)\n",
      " Command 'kate' from package 'kate' (universe)\n",
      "mate: command not found\n",
      "therwise: command not found\n",
      "bash: syntax error near unexpected token `('\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "#example configuration file for rhodobacter sphaeroides assembly from GAGE project (http://gage.cbcb.umd.edu)\n",
    "#DATA is specified as type {PE,JUMP,OTHER}=two_letter_prefix mean stdev fastq(.gz)_fwd_reads fastq(.gz)_rev_reads\n",
    "#NOTE that PE reads are always assumed to be  innies, i.e. --->  <---, and JUMP are assumed to be outties <------\n",
    ">; if there are any jump libraries that are innies, such as longjum\n",
    "p, specify them as JUMP and specify NEGATIVE mean\n",
    "#IT IS MANDATORY to supply some \n",
    "Illumina paired end or single end reads\n",
    "#reverse (R2) reads are optional for PE libraries and mandatory for JUMP libraries\n",
    "#any OTHER sequence data (454, Sanger, Ion torrent, etc) must be firstconverted into Celera Assembler compatible .frg files (see \n",
    "http://wgsassembler.sourceforge.com)and supplied as OTHER=file.frg; PAC\n",
    "BIO reads must be in a single FASTA file and supplied as PACBIO=reads.fa; \n",
    "NANOPORE reads must be in a single FASTA file and supplied as \n",
    "NANOPORE=reads.fa\n",
    "DATA\n",
    "PE= pe 180 20  /FULL_PATH/frag_1.fastq  /FULL_PATH/frag_2.fastq\n",
    "JUMP= sh 3600 200  /FULL_PATH/sho\n",
    "rt_1.fastq  /FULL_PATH/short_2.fastq\n",
    "OTHER=/FULL_PATH/file.frg\n",
    "PACBIO=reads.fa\n",
    "END\n",
    "PARAMETERS\n",
    "#this is k\n",
    "-\n",
    "mer size for deBruijn graph values between 25 and 101 are \n",
    "supported, auto will compute the optimal size based on the read data \n",
    "and GC content.  Do not\n",
    "set this longer than PE read length!!!\n",
    "GRAPH_KMER_SIZE=auto\n",
    "#set this to 1 for all Illumina\n",
    "-\n",
    "only assemblies\n",
    "#set this to 1 if you have less than 20x long reads (454, Sanger, \n",
    "Pacbio) and less than 50x CLONE coverage by Illumina, Sanger or 454 \n",
    "mate pairs\n",
    "#o\n",
    "therwise keep at 0\n",
    "USE_LINKING_MATES=1\n",
    "#this parameter is useful if you have too many jumping library mates. \n",
    "Typically set it to 60 for bacteria and something large (300) for \n",
    "mammals\n",
    "LIMIT_JUMP_COVERAGE = 60\n",
    "#these are the additional parameters to Celera \n",
    "Assembler.  do not \n",
    "worry about performance, number or processors or batch sizes \n",
    "--\n",
    "these \n",
    "are com\n",
    "puted automatically. for mammalian genomes\n",
    "do not set \n",
    "cgwErrorRate above 0.15!!!\n",
    "set cgwErrorRate=0.25 for bacteria\n",
    "CA_PARAMETERS = \n",
    "cgwErrorRate=0.15\n",
    "# number \n",
    "of cpus to use\n",
    "NUM_THREADS= 64\n",
    "#this is mandatory jellyfish hash size \n",
    "–\n",
    "10x the genome size is a good \n",
    "starting value\n",
    "JF_SIZE=100000000\n",
    "#this specifies if we do (1) or do not (0) want to trim long runs of \n",
    "homopolymers (e.g. GGGGGGGG) from 3' read ends, use \n",
    "it for high GC \n",
    "genomes\n",
    "DO_HOMOPOLYMER_TRIM=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
